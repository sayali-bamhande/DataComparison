import os
import pandas as pd
import time
from pyspark import SparkContext
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, concat, md5, lit
from datetime import datetime


def compare_files(file1_path, file2_path,bucket_name):
    # Read files into pandas DataFrames
    df1 = pd.read_csv(file1_path)
    df2 = pd.read_csv(file2_path)

    # Initialize an empty DataFrame to store output
    output_df = pd.DataFrame()

    # Iterate over rows
    for index, (row1, row2) in enumerate(zip(df1.itertuples(index=False), df2.itertuples(index=False))):
        # Initialize a dictionary to store differing columns
        diff_columns = {}

        # Iterate over columns
        for col_name, val1, val2 in zip(df1.columns, row1, row2):
            # Check if values are different
            if val1 != val2:
                # If different, add column to the dictionary
                diff_columns[col_name] = val1
                diff_columns[col_name + '_file2'] = val2
            else:
                # If same, add empty values
                diff_columns[col_name] = ''
                diff_columns[col_name + '_file2'] = ''

        # Append the row to the output DataFrame
        output_df = output_df.append(diff_columns, ignore_index=True)

    # Generate output file names with original file names and date-time
    output_filename_correct = f"{datetime.now().strftime('%Y%m%d_%H%M%S')}_{os.path.basename(file1_path)}"

    output_path = f'gs://{bucket_name}/Output/{output_filename_correct}'

    # Write output to CSV
    output_df.to_csv(output_path, index=False)


bucket_name = 'dumybucket123'
file1_path = 'gs://dumybucket123/Bigquery/BigQuery1000World_Bank_CO2.csv'
file2_path = 'gs://dumybucket123/DB2/BigQuery1000World_Bank_CO2_30%.csv'
output_path = f'gs://dumybucket123/Output/Discrepancy_Output.csv'
compare_files(file1_path,file2_path,bucket_name)